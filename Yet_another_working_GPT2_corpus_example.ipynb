{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yet another working GPT2 corpus example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/NLP/blob/master/Yet_another_working_GPT2_corpus_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3sMTD2rp0FP",
        "colab_type": "text"
      },
      "source": [
        "This was working code and the corpus file was live on 6/6/2020.\n",
        "\n",
        "> Please note that synthetic writing could be mean or otherwise terrible... My apologies in advance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53rjf4T1CHsR",
        "colab_type": "text"
      },
      "source": [
        "# Working example of GPT-2 using gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxTLEm3COGd",
        "colab_type": "text"
      },
      "source": [
        "The best method to use the OpenAI GPT-2 model is via gpt-2-simple. It works well enough. Some methods that I tried and that failed are commented out before it. The only method that seems to work is the gpt-2-simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4caFK3Zd_O7V",
        "colab_type": "code",
        "outputId": "443ba90e-3d8b-4b97-9d5a-b87fbceefd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "# !pip uninstall tensorflow -y \n",
        "# !pip uninstall tensorflow-gpu -y\n",
        "# !pip install tensorflow==1.14\n",
        "# !pip install tensorflow-gpu==1.4.1\n",
        "# !pip3 install tensorflow==1.12.0\n",
        "!pip install gpt-2-simple"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.18.4)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=23581 sha256=9d11652a79ae863cdb12bb16c3e3836fadf57060f09a335389743ca51f37a4ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkoGbGniCdrV",
        "colab_type": "text"
      },
      "source": [
        "You can now use the 1x TensorFlow selection and check what GPU you have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymSkRj-g-6Om",
        "colab_type": "code",
        "outputId": "e002acb3-59f1-4df6-8d96-338870fc2568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Sat Jun  6 17:48:32 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vGu6fSdDSVR",
        "colab_type": "text"
      },
      "source": [
        "This is our import of the GPT-2 we installed above. You get the warning that is the root of the GPT-2 problem noting that \"The TensorFlow contrib module will not be included in TensorFlow 2.0.'\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3ohher-i-U",
        "colab_type": "code",
        "outputId": "3cce4c19-73e1-47c8-9606-8f017d9b7b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import requests\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crNiRrIeDv5z",
        "colab_type": "text"
      },
      "source": [
        "This snippet is using the 355M model. The other model numbers are commented out above for later adventure "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CJwM6gA-rx5",
        "colab_type": "code",
        "outputId": "9f0d08b1-0c60-4f1c-c397-32d7c2242644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# !python3 download_model.py 124M\n",
        "# !python3 download_model.py 355M\n",
        "# !python3 download_model.py 774M\n",
        "# !python3 download_model.py 1558M\n",
        "model_name = \"355M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /content/models\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 724Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 91.3Mit/s]                                                   "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 355M model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching hparams.json: 1.05Mit [00:00, 954Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:10, 141Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 416Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 142Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 275Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCTbUpYov5Hr",
        "colab_type": "text"
      },
      "source": [
        "# Getting the corpus file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLNFaazwB2y",
        "colab_type": "text"
      },
      "source": [
        "To be able to go get files we need gdown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTGSoBt2v-0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "6d97dc8b-6375-4408-bdd7-443e2f9586c6"
      },
      "source": [
        "pip install gdown"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTWl3DxBwBz7",
        "colab_type": "text"
      },
      "source": [
        "We need to download the corpus file to do some training on it... This one gets the corpus I plan on using from a shareable Google Drive link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnme3geCwKZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "cc7ecc73-b5f1-4fe0-a7d2-c988b9798ebd"
      },
      "source": [
        "!gdown --id 1S7euXz9txzbrLTXJKl7IY5p6rd3jr0qn --output nlindahl.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S7euXz9txzbrLTXJKl7IY5p6rd3jr0qn\n",
            "To: /content/nlindahl.txt\n",
            "\r0.00B [00:00, ?B/s]\r3.96MB [00:00, 62.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmmAofSlxXPH",
        "colab_type": "text"
      },
      "source": [
        "Set the file name..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Pl03i2wSuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"nlindahl.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IguZVojTE3g_",
        "colab_type": "text"
      },
      "source": [
        "This will take a couple minutes to run... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvRR1T5jwTfV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2818eaa2-7ebf-43b7-da6a-bd332535db17"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.51s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 833647 tokens\n",
            "Training...\n",
            "[10 | 17.75] loss=3.02 avg=3.02\n",
            "[20 | 26.55] loss=3.20 avg=3.11\n",
            "[30 | 35.35] loss=1.65 avg=2.62\n",
            "[40 | 44.15] loss=3.30 avg=2.79\n",
            "[50 | 52.95] loss=2.95 avg=2.82\n",
            "[60 | 61.75] loss=3.11 avg=2.87\n",
            "[70 | 70.55] loss=2.67 avg=2.84\n",
            "[80 | 79.35] loss=2.49 avg=2.80\n",
            "[90 | 88.15] loss=2.85 avg=2.80\n",
            "[100 | 96.95] loss=3.06 avg=2.83\n",
            "[110 | 105.75] loss=3.09 avg=2.86\n",
            "[120 | 114.54] loss=3.20 avg=2.89\n",
            "[130 | 123.34] loss=3.02 avg=2.90\n",
            "[140 | 132.14] loss=2.75 avg=2.89\n",
            "[150 | 140.94] loss=2.78 avg=2.88\n",
            "[160 | 149.74] loss=2.91 avg=2.88\n",
            "[170 | 158.54] loss=3.12 avg=2.90\n",
            "[180 | 167.34] loss=2.56 avg=2.87\n",
            "[190 | 176.14] loss=2.83 avg=2.87\n",
            "[200 | 184.94] loss=2.68 avg=2.86\n",
            "======== SAMPLE 1 ========\n",
            " Those days are gone, so that was good to know. Sometimes even just having to make that move can be exciting. We are moving to a place of greater security and clarity. That may or may not involve some degree of personal freedom. I wasn't exactly sure about this when I made the trip, but this place just might be a place of some degree of personal peace. I spent a little bit of time thinking about the next step of this adventure, and that was a little bit of a dark moment. The path forward may have ended up being a little bit darker, and some of that is inevitable, and some of it was inevitable on account of the weather and the nature of the journey I am about to take. Sometimes I wonder if getting a few hours extra of sleep would improve my current situation. That was a pretty interesting thought that came out of the blue. \n",
            " Topics I’m thinking about today:\n",
            "1. A little video about the weather. That is pretty fun to watch, and some of this video is pretty interesting. It really just is. The weather has been good this morning, but we are in the middle of a new year  year of winter weather. People are worried about it now, and it is true that a lot of the weather may be making things more difficult to get things done. There are also a lot of cars stuck. People are talking and writing and talking all of the time. It seems like it does not feel much different from a good snow fall. People may be a lot more open to the internet than they used to be. \n",
            "2. It looks like a day at the office. It does not look like a day at work to me today. I’m trying to imagine a day at work that is so much different from the normal routine that is going on. At some point, a few folks will be sitting down at work and talking about politics. That happens all of the time, usually around 9:00 AM and the sun is shining down on that morning. In some ways, this day is probably not the day that I would spend writing or thinking about writing. It was probably not the day that I would be getting up and going out to the dance floor at this very moment. \n",
            "3. \n",
            "That one is the one that I’m thinking about right now. I’m thinking about the words that were written and how they could have been phrased in different ways. I’m thinking about all the things that could have been said, and what they could have meant. In that moment I got I’m going to go back and do my best to be better. It might just be the best way to go forward right now. We are all going to be on the same page at some point. \n",
            "I’m going to post a bunch of these blogs in one file tonight that will be stored in my blog folder. That will be the first thing I’m doing this evening. I’m only at 2% and my blog is only 0.6% of everything I’m writing. You will see a ton of the same things over the course of the rest of the weekend. \n",
            "Let the weekend begin. Take things one day at a time. It might be the best way to start planning your writing sessions. We have a few hours of daylight at the moment. That is enough to make reading the night before not be a fun challenge, but it isn’t nearly enough to make planning for the night a real challenge. That next 30 minutes might just be the time where this entire blog post really starts to be a little bit of a slog. Every question is a little bit detailed and maybe a little bit intriguing for me to ask anything specific. \n",
            "That means that I have not answered every question that will be asked about this writing session. That is not really the reason my life will have continued on this road trip for two days now. My life really started to end about a month and a half ago. My life just started to spin out of control. My apartment was broken into and taken. A bunch of belongings were taken, and even my laptop computer was missing. That was really the last straw of the events. My life just spun out of control in ways that probably do not benefit the writing sessions next week. That is something that has been a part of the blog since it started. \n",
            "It all just kind of... slowed down the night before this journey started and it is really hard to say exactly what just stopped it. That would make for a great story to tell next night, but it is not exactly happening. It has been kind of a blur in the last 30 minutes. \n",
            "The day that started with\n",
            "\n",
            "[210 | 212.16] loss=2.57 avg=2.85\n",
            "[220 | 220.96] loss=2.60 avg=2.83\n",
            "[230 | 229.75] loss=3.08 avg=2.85\n",
            "[240 | 238.55] loss=2.94 avg=2.85\n",
            "[250 | 247.34] loss=3.12 avg=2.86\n",
            "[260 | 256.14] loss=2.76 avg=2.86\n",
            "[270 | 264.93] loss=3.12 avg=2.87\n",
            "[280 | 273.72] loss=3.00 avg=2.87\n",
            "[290 | 282.52] loss=2.77 avg=2.87\n",
            "[300 | 291.34] loss=2.98 avg=2.87\n",
            "[310 | 300.14] loss=2.60 avg=2.86\n",
            "[320 | 308.93] loss=2.75 avg=2.86\n",
            "[330 | 317.73] loss=2.78 avg=2.86\n",
            "[340 | 326.52] loss=2.59 avg=2.85\n",
            "[350 | 335.31] loss=2.32 avg=2.83\n",
            "[360 | 344.11] loss=2.80 avg=2.83\n",
            "[370 | 352.90] loss=2.40 avg=2.82\n",
            "[380 | 361.69] loss=2.70 avg=2.81\n",
            "[390 | 370.49] loss=3.15 avg=2.82\n",
            "[400 | 379.28] loss=2.55 avg=2.81\n",
            "======== SAMPLE 1 ========\n",
            " whole, my current setup includes three 2TB Toshiba Satellite 7200RPM hard drives. That should do just fine to be used for a few of my other drives. Right now I am really digging into the new Lenovo Thinkpad 13.\n",
            "My other current setup has two 4TB Samsung 64GB SSD's. Both of them are running OS X Mountain Lion. My Chromebook has a copy available from Google Play. \n",
            "Topic 3: Writing\n",
            "Today I picked up a copy of John Paul Newman’s The Collected Tales of John Paul Newman from my local Barnes and Noble bookstore. I had it ready to go before getting this copy of Steve Jobs. I have been really interested in reading the book. I had been hoping to pick up an audio book, but that never materialized or my Google Play Books subscription was cancelled. This one sounded like a good idea. Now it may be the best use of my time. Today is really important. Maybe today is a day that my time is being taken up in preparation for what is next. Maybe tomorrow is going to be interesting.\n",
            "In my opinion, writing today will go to the front burner…\n",
            "Okay, so I just needed to write a few paragraphs on my next computer build, the ASUS TUF Mark 1. It will be available in December. I’m not sold on the purchase of the computer. It appears very expensive. Not being sold on something is always a challenge of course. However, just maybe getting back to the point of using the computer for something more than what it cost would be a recipe for disaster. I’ll be watching the new Star Trek episode “First Contact”. It has been a while since I last watched a Star Trek episode. Star Trek Discovery seems to have been rather interesting. I tried to watch it on a Google Pixel 2 XL Chromebook.\n",
            "Today might just be my time to pick up some headphones and write some words… I’m really going to miss my old Nokia Lumia 1020. It is still alive and well in my opinion.\n",
            "Today might be a big day for my computer builds and maybe of my writing efforts. It will be a day of excitement…\n",
            "Topic 4: The best possible Christmas Tree\n",
            "I’m getting ready for my 40th birthday. All of my Christmas tree efforts have ended up costing about 2,000-1,000 dollars. All of my tree plans are still running. I have been thinking about the potential of going to a local hardware store to buy a new desk mount. This new mount has some really good design. After watching my first HD cable, I was really curious to see how it worked. Now that I know, it does not look cheap or anything like my Sony Vaio VGN-T200K monitor. I have been reading the reviews online about cables and monitors and thinking about my plans for the future.\n",
            "My new ASUS VGN-T200T monitor is finally setup and running. Now is the time to have a look around.\n",
            "Topic 5: Writing 2,000 words in a day\n",
            "After spending a night last night with a ton of coffee we are going to sit down to write something for 10 hours. Tonight we are going to try to write 1,000 words (plus half a word per hour) in a day for the morning. That was easy enough. I’m not sure if doing it today would be easier, but maybe it would be worth a shot. The first 10 minutes of the morning will be very exciting and fun. We went to Starbucks. Starbucks has 2 locations open today. That was one of the reasons I stopped ordering lattes. I’ll probably be ordering a latte right now. That is just a better way to go than some of the other coffee pods on the market.\n",
            "Today I decided to just put in my word processor and let it finish writing a few sentences at a time. Sometimes I sit down at the keyboard and just write with words rather than spend an hour watching YouTube videos on a laptop or watching a movie. I’m not sure if that will be a good or bad thing for my productivity, but for the most part I just spend the time working on whatever blog posts I think are interesting. That is how I’m probably going to move forward. Writing a few good blog posts of prose will take the lead and produce good prose. My thoughts just kept spinning around the world and that pretty much meant a total stop at Starbucks. I’m not sure if my new mount would benefit from spending a day in that building.\n",
            "The next day started off as the last day of Christmas. No matter how hard I went with work today was about to be interrupted by the Christmas tree.\n",
            "This morning I started out with a single coffee and a cup of tea. For the first time today, I’m going to drink a drink of coffee. I’m not entirely sure it will work out. Maybe I will just have to figure out what\n",
            "\n",
            "[410 | 404.14] loss=2.74 avg=2.81\n",
            "[420 | 412.94] loss=3.01 avg=2.82\n",
            "[430 | 421.74] loss=1.99 avg=2.79\n",
            "[440 | 430.53] loss=2.88 avg=2.80\n",
            "[450 | 439.32] loss=3.13 avg=2.81\n",
            "[460 | 448.12] loss=3.05 avg=2.81\n",
            "[470 | 456.91] loss=2.76 avg=2.81\n",
            "[480 | 465.71] loss=2.83 avg=2.81\n",
            "[490 | 474.50] loss=3.16 avg=2.82\n",
            "[500 | 483.30] loss=2.54 avg=2.81\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 498.93] loss=3.01 avg=2.82\n",
            "[520 | 507.72] loss=2.64 avg=2.81\n",
            "[530 | 516.53] loss=3.12 avg=2.82\n",
            "[540 | 525.32] loss=2.42 avg=2.81\n",
            "[550 | 534.11] loss=1.91 avg=2.79\n",
            "[560 | 542.90] loss=1.74 avg=2.77\n",
            "[570 | 551.70] loss=2.74 avg=2.77\n",
            "[580 | 560.49] loss=2.97 avg=2.77\n",
            "[590 | 569.29] loss=1.64 avg=2.74\n",
            "[600 | 578.08] loss=2.57 avg=2.74\n",
            "======== SAMPLE 1 ========\n",
            ". But with these guys it has been a very unique season to watch.\"\n",
            "Swing Day\n",
            "April 15, 2009 Categories: Sports\n",
            "For some reason the swing of the morning doesn�t translate from one hour to the next. The same can be said for every day. This morning, the Cubs have gone from a solid game against the Braves to a solid game against the Braves. The game really was close, but things just did not come together during the third inning. One thing that is not in question is that the Cubs have a rotation that is not doing particularly well in terms of quality pitching against quality competition. Today will be a day where things just do not appear to have clicked on the surface or when something catches your attention in the moment. The team is facing a 4 in 6 win possibility going up next week at Wrigley in Chicago.\n",
            "Swing Day Update: The game was on on with about a half hour remaining against the Braves. It was the first half in Chicago since the rain had stopped last night. The Cubs just had enough time to run out the clock. The team is on a bye this week because of work and the Cubs have had two extra days off this week. The Cubs are just one game out of the playoffs for the second straight year.\n",
            "For the eighth night in a row, the Cubs are playing catch up after missing the past few games. This is not necessarily a good sign for the team. The Cubs are sitting pretty with a 1-0 series lead. They are probably within striking range of making it to 10,000 games. This is also the first time in a long time that the Cubs are not in the World Series. This game could go either way. The ball is going to be a strike against either team. It is something that will probably be close enough to being exciting. The game looks pretty darn good in the dugout. After watching some of the highlights from the game, I just do not enjoy the view from the visitors dugout that does not have the lighting effects. Maybe it is a case of the light reflecting off the grass, rocks, and of course a few trees.\n",
            "Tonight at some point this weekend the entire baseball world will have something to watch and marvel at. You can probably guess that my thoughts on that topic have been mixed for some time. I might be having a hard time putting it in a box or a large file on my desk as a writing topic for the year. It is also possible that I will never really get to the point of figuring it out, but just for the record the box that contains it is in the basement. I do not need that box to be of value as I do not actually own anything other than my collection of sports trading cards. In terms of pure economic theory that probably just means that the value of that trade card is the money it gives away from one set in a pool of value. It kind of makes me laugh every time I see it in the mail because people really are just not buying the baseball cards. They are a mass produced market. Baseball cards are more expensive than you would think and they are a mass manufactured product to make sure the mass demand out there is greater than the supply problem. That is a thing that you can measure in terms of the actual number of people being exposed to something that really looks like what is being sold. Baseball cards are a product that exists out in public as a commodity of value for those with the currency to pay for goods that are being presented over a window into a product that does not really function as a function of something that actually stands on it side by side and shows value in relation to something else at the same time being used by a different group of people to display a profit model in a way that is just pure mass production at this point. At some point, it will be sold as vanity and then people will be more willing to put up the cash for it. \n",
            "You can probably tell that my overall rating for the Baseball Cards of America trade show is pretty good. The cards are all in pretty decent condition, but they are not being sold out everywhere. Some of the older ones are in pretty good shape, but they are all in pretty good nick in the market of resellers. To that extent I do feel pretty good about the quality of the Baseball Cards of America trade show and have not given them any particular dislike. They are doing a great job of bringing baseball cards in reasonably priced in a pretty decent boxes for what looks like a large retail center selling a ton of baseball cards. I really do enjoy that they sell from large boxes. It is one of those things you can see the prices go up and down as the box size goes up. A decent trade show you can easily get on in at the corner and see the prices go up and down before you even enter the show. \n",
            "The game went in very well for the first inning with one run as a team, but then things did not go in the right direction for the second inning. In between innings of\n",
            "\n",
            "[610 | 603.02] loss=1.93 avg=2.72\n",
            "[620 | 611.82] loss=2.89 avg=2.73\n",
            "[630 | 620.62] loss=2.23 avg=2.72\n",
            "[640 | 629.41] loss=1.59 avg=2.69\n",
            "[650 | 638.20] loss=2.39 avg=2.69\n",
            "[660 | 647.00] loss=2.55 avg=2.68\n",
            "[670 | 655.79] loss=2.04 avg=2.67\n",
            "[680 | 664.59] loss=1.68 avg=2.65\n",
            "[690 | 673.38] loss=2.75 avg=2.65\n",
            "[700 | 682.17] loss=2.52 avg=2.65\n",
            "[710 | 690.97] loss=2.88 avg=2.65\n",
            "[720 | 699.76] loss=1.91 avg=2.64\n",
            "[730 | 708.55] loss=2.62 avg=2.64\n",
            "[740 | 717.35] loss=2.75 avg=2.64\n",
            "[750 | 726.16] loss=1.56 avg=2.62\n",
            "[760 | 734.95] loss=2.32 avg=2.62\n",
            "[770 | 743.75] loss=3.05 avg=2.62\n",
            "[780 | 752.54] loss=1.85 avg=2.61\n",
            "[790 | 761.33] loss=2.39 avg=2.61\n",
            "[800 | 770.13] loss=1.97 avg=2.59\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "The problem with the new system though is that with only two tiers of pricing the system can only be good for people looking to buy a new television. This means that the system is broken on the one tier of television versus the system is broken on the additional tiers of television. It is possible in theory that a new system would reduce the prices on the standard tier of television. After looking at the entire system the new system does not look good on the box and screen. The first two television tiers look good from the outside. They only have three channels, but the channels look pretty good on the television. A good three channel television program could cost between five hundred and seven25 for the full season. That is a significant difference. The system allows for a television to have a total of about three dozen shows on at any time.\n",
            "This is not going to change until somewhere between 2015 and 2017. That is where things will need to get interesting. There will be many interesting changes that may or may not involve television shows. That is very possible at this point in the current technology. At its best the television could last a decade. Television can become overwhelming after awhile. Sure that is what happened to me during the last two years. I wanted to make the best of everything that was happening in a world of uncertainty. Everything that could possibly be possible and hopefully it happened. Maybe now is really the right time to take another step in the journey toward being a better human being. I started the journey with the mindset of a great teacher. My plan involves engaging in constructive problem solving. For some students doing homework it can be empowering to look at homework. You can only review and understand a specific set of research studies so much. Your interactions with someone will change over time. That could be positive or negative. You can only ask for or give up information until the journey is complete.\n",
            "As you can tell the plan does not include football games this year. I’m going to try to write up an outline of a typical Sunday afternoon schedule this weekend. You can expect a lot of blog posts about planning and life and everything else this weekend. The best time to travel to a new place is during the process of writing. Some thoughts will probably come to the forefront of the mind. You may see my new Samsung Galaxy Note 7 on our television screen this weekend. The device will be the only thing watching this year. It will be the first device to get a modern wireless processor. This is going to be an interesting year. I hope and pray that the end of this year coincides with the start of the next year. This will be a wild ride. We will see to find out this weekend. It won’ very soon.\n",
            "Week 17, 2018\n",
            "January 5, 2018 Categories: Writing\n",
            "It was about time to update the blog post at this point today. Things did not end up getting to the point of being timely. Sure that is part of writing. It happens. It was about time to take the next step in the journey toward moving things forward. Something has to give. Things are full of possibilities; after all that is where the joy is. In all honesty, this could be about a year of writing or an entire year of work. We will have to wait until after the NFL is gone to truly see the end of the Peyton Manning era. I’m not ready to wait.\n",
            "Writing can be a lot of things. Today was a day of reflection. It was about making sure we stay focused and focused on the journey. The journey from tomorrow to now is full of promise. We must strive to move forward and move forward together. That is what moving forward feels like. It is also where we look back at things and realize how close we came together in this moment. We are all a part of the same humanity and that is what makes our current moment worth living. It is a time for optimism, for change, and for the pursuit of progress. That is pretty much the key to moving forward. It is a chance to move toward something greater. The path to a goal can be very long and that is where the heart of hope resides. We must find the strength to move forward alone. We are surrounded by the capacity to move together. It is a promise made with the full knowledge that we will be together. Things could still get weirder.\n",
            "We make the choices we make every day. That is one of the things that we do every day. We make the choices that lead to better for our children. We make the choices we make every day to be better for our children. It was probably a better idea to not ask this year to stay at home and enjoy the NFL ratings. It was probably a better idea to do nothing and to watch the broadcast broadcast of the game without cable or satellite television. The networks have chosen to air more football games and are paying much more money to their rights holders than to us. Some folks might say that this will be the year that cable television subscribers lose their cable subscriptions.\n",
            "\n",
            "[810 | 794.89] loss=2.09 avg=2.58\n",
            "[820 | 803.69] loss=1.73 avg=2.57\n",
            "[830 | 812.48] loss=3.14 avg=2.58\n",
            "[840 | 821.28] loss=2.79 avg=2.58\n",
            "[850 | 830.08] loss=3.16 avg=2.59\n",
            "[860 | 838.87] loss=1.97 avg=2.58\n",
            "[870 | 847.66] loss=2.28 avg=2.58\n",
            "[880 | 856.45] loss=1.88 avg=2.57\n",
            "[890 | 865.25] loss=2.84 avg=2.57\n",
            "[900 | 874.03] loss=1.81 avg=2.56\n",
            "[910 | 882.84] loss=1.82 avg=2.54\n",
            "[920 | 891.63] loss=2.12 avg=2.54\n",
            "[930 | 900.42] loss=2.78 avg=2.54\n",
            "[940 | 909.23] loss=2.37 avg=2.54\n",
            "[950 | 918.04] loss=2.25 avg=2.53\n",
            "[960 | 926.84] loss=3.10 avg=2.54\n",
            "[970 | 935.63] loss=1.98 avg=2.53\n",
            "[980 | 944.43] loss=3.11 avg=2.54\n",
            "[990 | 953.22] loss=2.43 avg=2.54\n",
            "[1000 | 962.01] loss=2.39 avg=2.54\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcPgaPmDjWxe",
        "colab_type": "text"
      },
      "source": [
        "# Let's make some outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPccwlUYzMvp",
        "colab_type": "text"
      },
      "source": [
        "This prompt will generate text from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si4k2vU7wTi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "cc52e004-9763-44a1-af5f-2e556c11a275"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A video has emerged of a group of people hanging out in a park in the Czech Republic. It was pretty much just inside the park. I watched it for about 20 minutes and then stopped. The video post has a lot of stationary hanging around. They seem to be doing something in the park. Maybe they are just hanging out a bit and watching the sun come up. That seems to be what is going on. It is a pretty decent video. I have gone through my collection of old popular science videos. It is a good one. One of the things that I have to work on is creating and maintaining a video archive of my favorite science. I have a lot of videos in my YouTube library. Some of them are from my college years. Some of them are from my graduate school classes. I will probably be working on getting rid of them. \n",
            "Upcoming 2018 Writing Topics:\n",
            "— Recap of all the video camera equipment I have owned\n",
            "— All the promise and failures of my first Sony camcorder\n",
            "— That one with a roadtrip to Florida\n",
            "— Applied AI: A use case based exploration\n",
            "— My ode to minor league hockey\n",
            "— Progress within general AI vs. specialized use cases\n",
            "— My review of the ASUS C101P Chromebook\n",
            "— On leadership and the modern workplace\n",
            "— Short story “The Knock”\n",
            "Feel free to leave topic suggestions in the comment section. I keep a ton of them in my daily writing schedule.\n",
            "Day 5 of my writing journey\n",
            "January 5, 2018 Categories: The 3,000 word a day writing habit\n",
            "\n",
            "2018: Day 5 of 365 writing 3,000 words or whatever it is\n",
            "Word count 14,886 of 1,000,000\n",
            "Dear Reader,\n",
            "Day 5 of my writing journey was going to be full of a lot of narrative writing. My next writing session was going to be dedicated to writing a short story. It was going to be epic. That is not how it ended up. It was not even a story. I just wrote something that was not a story. That is not all that interesting of a thing to happen, but it happened. \n",
            "Dr. Nels Lindahl\n",
            "Broomfield, Colorado\n",
            "Written on my Storm Stryker PC and or my ASUS C101P Chromebook\n",
            "Day 6 of my writing journey\n",
            "February 6, 2018 Categories: The 3,000 word a day writing habit\n",
            "\n",
            "Word count 18,711 of 1,000,000\n",
            "Dear Reader,\n",
            "Good morning after visiting the Denver Museum of Nature & Science I had some coffee. It was not a regular cup of coffee, but it was nice. Things were going to get going. I’m not sure if this morning’s adventure will yield 1,000,000 words, but it is a good start. At the end of the adventure I’m going to give it one more shot and see what happens. That seems to be the direction that my writing is going at the moment. \n",
            "Today for the first time since we moved to the house I complained about the heat. That is a real thing. I know it is not a trivial thing to worry about. The house was warm, but I wanted to sit down and complain about it. That is a healthy thing to do. I’m worried about how the pipe that was inside my Storm Stryker computer is holding up. I’m guessing it should stay put and take a hit from the heat. I think the thing could be holding up just fine. I’m sure the heat is not that unusual from the month of February. My Corsair Cube case does not have a fan as large as that. It is just smaller. It is just a case with a removable top, which is great for running a computer. \n",
            "I have been thinking about just replacing my PC with a new motherboard. That is just one way to go about things. I could keep building out my first computer and setting it up to do some computation. It could be fun to just write for a bit and see what happens. We could do some kind of experiment to see if writing everyday is productive. That is one way to go about it. It might just be a few years and I will see a return to my typewriter. \n",
            "Dr. Nels Lindahl\n",
            "Broomfield, Colorado\n",
            "Written on my Storm Stryker PC and or my ASUS C101P Chromebook\n",
            "March 4, 2018 Categories: The 3,000 word a day writing habit\n",
            "\n",
            "Word count 22,184 of 1,000,000\n",
            "Dear Reader,\n",
            "Today was one of those days. It was a day where a cup of coffee was not going to be enough. That happens. You really ought to have a backup in the event of a major emergency. I’m not sure if that was the case this morning or it could be a day where I am already thinking about the weather. It could be a day when the weather is not very\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6YAVDBM1zQj",
        "colab_type": "text"
      },
      "source": [
        "You can change the prefix text, but it spews nonsense so far..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6wo9vY-xuO",
        "colab_type": "code",
        "outputId": "ddb9c82b-a1ad-45f9-de32-2c5f698e5f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "single_text = gpt2.generate(sess, prefix=\"What makes Phil happy\")\n",
        "print(single_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What makes Phil happy?\n",
            "October 30, 2005 Categories: Weblog Updates\n",
            "Phil has been writing and blogging for over a decade. In that time he has written both fiction and non-fiction books. In the middle of the last decade, Phil wrote a book entitled, “The 7 Habits of Highly Effective People.” I purchased the book from the publisher at a huge discount. I am glad that Phil has taken the time to work on improving the book. In the end, the book is not the only thing that Phil does. Phil is still working on writing 3,000 words per day.\n",
            "Phil writes on a variety of subjects. Sometimes it is easy to just write about the things you want to write about. Sometimes writing about the things you want to write about takes a certain degree of skill and persistence. During my journey, I have learned a great deal about people. I have learned a great deal about life and the universe.\n",
            "Pictures of the day\n",
            "October 31, 2005 Categories: Weblog Updates\n",
            "My Nalgene nasal spray pack is still working.\n",
            "People seem to be asking about my new Sony Bravia television.\n",
            "October 32, 2005 Categories: Weblog Updates\n",
            "When I purchased my (now discontinued) Sony Bravia television I did not know that Sony would retire the wireless DVR. I never thought that the Sony video camera company would sell a television with a DVR capability. I have been looking at the Sony Bravia as a television replacement. I have been using the live video chat feature for years. I am not sure if you can put the video back into the cloud and just make it available for anyone to watch.\n",
            "I am still thinking about my new television.\n",
            "I am still thinking about the future of the Sony Bravia television.\n",
            "I am considering buying a new television\n",
            "October 31, 2005 Categories: Weblog Updates\n",
            "I am considering purchasing a new television.\n",
            "I have been watching the CBS television network.\n",
            "October 31, 2005 Categories: Technology\n",
            "I have not been watching the CBS television network.\n",
            "November 5, 2005 Categories: Technology\n",
            "I have been looking at the Samsung Galaxy Tab S.\n",
            "I am trying to figure out what to do with my vast collection of television shows.\n",
            "I have been thinking about new technologies.\n",
            "November 6, 2005 Categories: Technology\n",
            "I am considering purchasing a new television.\n",
            "I have been reading the book, “The 7 Habits of Highly Effective People.”\n",
            "I have been reading the book, “The 7 Habits of Highly Effective People: The 10,000-Day Test.” I have been reading the book for a couple of days.\n",
            "In the midst of all of this thought and consideration of technology I am thinking about buying a new television.\n",
            "photo by jeremyboom\n",
            "November 6, 2005 Categories: Technology\n",
            "I feel like my Nalgene nasal spray is working.\n",
            "November 6, 2005 Categories: Education\n",
            "I am considering purchasing a new television.\n",
            "I have been working on an article on television.\n",
            "November 6, 2005 Categories: News\n",
            "I have been working on several articles.\n",
            "My Nalgene nasal spray is still working.\n",
            "November 7, 2005 Categories: Writing\n",
            "I have been taking the time to write some more words on the Nalgene nasal spray.\n",
            "November 7, 2005 Categories: Weblog Updates\n",
            "I have been working on getting my Nalgene nasal spray to last longer.\n",
            "I have been reading the book, “The 7 Habits of Highly Effective People.”\n",
            "November 8, 2005 Categories: Weblog Updates\n",
            "I have not been able to read all of the articles.\n",
            "I have been trying to figure out what to write next.\n",
            "November 8, 2005 Categories: Writing\n",
            "I have been thinking about writing some more words on the Nalgene nasal spray.\n",
            "I have been reading the book, “The 7 Habits of Highly Effective People.”\n",
            "November 9, 2005 Categories: Weblog Updates\n",
            "I have been working on writing some more words on the Nalgene nasal spray.\n",
            "I have been reading the book, “The 7 Habits of Highly Effective People.”\n",
            "November 10, 2005 Categories: Writing\n",
            "I am still considering writing a book about my days.\n",
            "November 11, 2005 Categories: Writing\n",
            "I have been reading the book, “Think Like a Champion.”\n",
            "October 31, 2006 Categories: Writing\n",
            "I have been working on two different books about my days.\n",
            "I have been reading articles related to writing about the daily grind.\n",
            "November 5, 2005 Categories: Weblog Updates\n",
            "I have been reading the book, “Think Like a Champion.”\n",
            "November 5, 2005 Categories: Writing\n",
            "I have been reading the book, “Think Like a Champion.”\n",
            "November 5, 2005 Categories: Weblog Updates\n",
            "I have been reading the book, “Think Like a Champion.” \n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}