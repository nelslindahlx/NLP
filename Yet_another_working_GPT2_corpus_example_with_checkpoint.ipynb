{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Yet another working GPT2 corpus example with checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/NLP/blob/master/Yet_another_working_GPT2_corpus_example_with_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3sMTD2rp0FP",
        "colab_type": "text"
      },
      "source": [
        "This was working code and the corpus file was live on 6/7/2020.\n",
        "\n",
        "> Please note that synthetic writing could be mean or otherwise terrible... my apologies in advance for what is generated from the corpus... \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53rjf4T1CHsR",
        "colab_type": "text"
      },
      "source": [
        "# Working example of GPT-2 using gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxTLEm3COGd",
        "colab_type": "text"
      },
      "source": [
        "The best method to use the OpenAI GPT-2 model is via gpt-2-simple. It works well enough. Some methods that I tried and that failed are commented out before it. The only method that seems to work is the gpt-2-simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4caFK3Zd_O7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip uninstall tensorflow -y \n",
        "# !pip uninstall tensorflow-gpu -y\n",
        "# !pip install tensorflow==1.14\n",
        "# !pip install tensorflow-gpu==1.4.1\n",
        "# !pip3 install tensorflow==1.12.0\n",
        "!pip install gpt-2-simple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkoGbGniCdrV",
        "colab_type": "text"
      },
      "source": [
        "You can now use the 1x TensorFlow selection and check what GPU you have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymSkRj-g-6Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vGu6fSdDSVR",
        "colab_type": "text"
      },
      "source": [
        "This is our import of the GPT-2 we installed above. You get the warning that is the root of the GPT-2 problem noting that \"The TensorFlow contrib module will not be included in TensorFlow 2.0.'\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3ohher-i-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import requests\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crNiRrIeDv5z",
        "colab_type": "text"
      },
      "source": [
        "This snippet is using the 124M model. The other model numbers are commented out above for later adventure. I tried the 774M model and that failed.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CJwM6gA-rx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 download_model.py 124M\n",
        "# !python3 download_model.py 355M\n",
        "# !python3 download_model.py 774M\n",
        "# !python3 download_model.py 1558M\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /content/models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Pave0AIM9oX"
      },
      "source": [
        "This snippet is using the 355M model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XIlZDCxQM9oY",
        "colab": {}
      },
      "source": [
        "# !python3 download_model.py 124M\n",
        "# !python3 download_model.py 355M\n",
        "# !python3 download_model.py 774M\n",
        "# !python3 download_model.py 1558M\n",
        "model_name = \"355M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /content/models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCTbUpYov5Hr",
        "colab_type": "text"
      },
      "source": [
        "# Getting the corpus file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLNFaazwB2y",
        "colab_type": "text"
      },
      "source": [
        "To be able to go get files we need gdown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTGSoBt2v-0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gdown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTWl3DxBwBz7",
        "colab_type": "text"
      },
      "source": [
        "We need to download the corpus file to do some training on it... This one gets the corpus I plan on using from a shareable Google Drive link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnme3geCwKZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown --id 1S7euXz9txzbrLTXJKl7IY5p6rd3jr0qn --output nlindahl.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmmAofSlxXPH",
        "colab_type": "text"
      },
      "source": [
        "Set the file name..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Pl03i2wSuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"nlindahl.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IguZVojTE3g_",
        "colab_type": "text"
      },
      "source": [
        "Let's run the 355M model. This will take about 20 minutes to run... \n",
        "\n",
        "*   This does have 2 warnings right now for tf.where in 2.0 and a deprication warning for get_backward_walk_ops\n",
        "*   sample_every was increased to 100 vs. 200. This makes it easier to watch. it has no other beneift\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvRR1T5jwTfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=100,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgSklinhVVUK",
        "colab_type": "text"
      },
      "source": [
        "Or instead of the 355M you could run the 124M model. This will take about *20* minutes to run... \n",
        "\n",
        "*   This does have 2 warnings right now for tf.where in 2.0 and a deprication warning for get_backward_walk_ops\n",
        "*   sample_every was increased to 100 vs. 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i1_NTCU1NIB_",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run2',\n",
        "              print_every=10,\n",
        "              sample_every=100,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcPgaPmDjWxe",
        "colab_type": "text"
      },
      "source": [
        "# Let's make some outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPccwlUYzMvp",
        "colab_type": "text"
      },
      "source": [
        "This prompt will generate text from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si4k2vU7wTi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6YAVDBM1zQj",
        "colab_type": "text"
      },
      "source": [
        "You can change the prefix text, but it spews nonsense so far..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6wo9vY-xuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_text = gpt2.generate(sess, prefix=\"What makes Phil happy\")\n",
        "print(single_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78r0MRiZLlNb",
        "colab_type": "text"
      },
      "source": [
        "Another generate with a prefix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAqAaTvyLmhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Today I wrote about\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsG7_mDsFmuY",
        "colab_type": "text"
      },
      "source": [
        "# Messing around with doing a zip of the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Aw3iAWF_kD",
        "colab_type": "text"
      },
      "source": [
        "Doing one shot trainign worked well enough above. I'm now going to try to train this with more shots and save the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYLk2kCzFnB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r /content/models/file.zip /content/models/355M"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}